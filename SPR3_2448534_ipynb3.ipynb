{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"nodal-ivy-473710-r7-afc93e14c3af.json\"\n"
      ],
      "metadata": {
        "id": "wEzwZYe7H4Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -qq && apt-get install -y -qq ffmpeg\n",
        "!pip install --quiet openai-whisper vosk google-cloud-speech pydub jiwer pandas\n"
      ],
      "metadata": {
        "id": "7KGS_O6cGmwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a942940-d435-47b5-e044-31f0f3bd9d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, io, json, sys\n",
        "from google.cloud import speech_v1 as speech\n",
        "from google.oauth2 import service_account\n",
        "from google.api_core import exceptions as google_exceptions\n",
        "import whisper\n",
        "from vosk import Model, KaldiRecognizer\n",
        "from pydub import AudioSegment\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from jiwer import wer\n"
      ],
      "metadata": {
        "id": "7BPvQzRlIgpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WHISPER_MODEL = \"small\"\n",
        "VOSK_MODEL_PATH = \"vosk-model-small-en-us-0.15\"\n",
        "\n",
        "if not os.path.exists(VOSK_MODEL_PATH):\n",
        "    print(\"Downloading Vosk small model (approx ~50 MB)...\")\n",
        "    !wget -q https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip\n",
        "    !unzip -q vosk-model-small-en-us-0.15.zip\n",
        "    !rm -f vosk-model-small-en-us-0.15.zip\n",
        "    print(\"Vosk model ready.\")\n",
        "\n",
        "def convert_to_wav(in_path, out_path=\"converted.wav\"):\n",
        "    \"\"\"Convert any audio to 16kHz mono WAV (LINEAR16) which is suitable for Vosk & Google.\"\"\"\n",
        "    audio = AudioSegment.from_file(in_path)\n",
        "    audio = audio.set_frame_rate(16000).set_channels(1)\n",
        "    audio.export(out_path, format=\"wav\")\n",
        "    return out_path\n"
      ],
      "metadata": {
        "id": "ecEGkqe7Ihkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_whisper(wav_path, model_name=WHISPER_MODEL):\n",
        "    print(\"Recognizing with Whisper...\")       # During recognition message\n",
        "    try:\n",
        "        model = whisper.load_model(model_name)\n",
        "        res = model.transcribe(wav_path)\n",
        "        text = (res.get(\"text\") or \"\").strip()\n",
        "        if not text:\n",
        "            msg = \"Speech Recognition could not understand audio. Please try speaking more clearly.\"\n",
        "            print(msg)\n",
        "            return msg\n",
        "        print(\"Speech successfully converted to text! (Whisper)\")\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        err = f\"[Whisper error] {e}\"\n",
        "        print(err)\n",
        "        return err\n",
        "\n",
        "def run_vosk(wav_path, model_path=VOSK_MODEL_PATH):\n",
        "    print(\"Recognizing with Vosk...\")\n",
        "    try:\n",
        "        import wave\n",
        "        wf = wave.open(wav_path, \"rb\")\n",
        "        model = Model(model_path)\n",
        "        rec = KaldiRecognizer(model, wf.getframerate())\n",
        "        rec.SetWords(True)\n",
        "        final_text = \"\"\n",
        "        while True:\n",
        "            data = wf.readframes(4000)\n",
        "            if len(data) == 0:\n",
        "                break\n",
        "            if rec.AcceptWaveform(data):\n",
        "                j = json.loads(rec.Result())\n",
        "                final_text += \" \" + j.get(\"text\", \"\")\n",
        "        j = json.loads(rec.FinalResult())\n",
        "        final_text += \" \" + j.get(\"text\", \"\")\n",
        "        final_text = final_text.strip()\n",
        "        if not final_text:\n",
        "            msg = \"Speech Recognition could not understand audio. Please try speaking more clearly.\"\n",
        "            print(msg)\n",
        "            return msg\n",
        "        print(\"Speech successfully converted to text! (Vosk)\")\n",
        "        return final_text\n",
        "    except Exception as e:\n",
        "        err = f\"[Vosk error] {e}\"\n",
        "        print(err)\n",
        "        return err\n",
        "\n",
        "def run_google_cloud(wav_path, client):\n",
        "    print(\"Recognizing with Google Cloud Speech-to-Text...\")\n",
        "    try:\n",
        "        with open(wav_path, \"rb\") as f:\n",
        "            content = f.read()\n",
        "        audio = speech.RecognitionAudio(content=content)\n",
        "        config = speech.RecognitionConfig(\n",
        "            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,\n",
        "            sample_rate_hertz=16000,\n",
        "            language_code=\"en-US\",\n",
        "            enable_automatic_punctuation=True,\n",
        "            model=\"default\"\n",
        "        )\n",
        "        response = client.recognize(config=config, audio=audio)\n",
        "        transcripts = []\n",
        "        for result in response.results:\n",
        "            # take top alternative\n",
        "            transcripts.append(result.alternatives[0].transcript)\n",
        "        transcript = \" \".join(transcripts).strip()\n",
        "        if not transcript:\n",
        "            msg = \"Speech Recognition could not understand audio. Please try speaking more clearly.\"\n",
        "            print(msg)\n",
        "            return msg\n",
        "        print(\"Speech successfully converted to text! (Google Cloud)\")\n",
        "        return transcript\n",
        "    except google_exceptions.GoogleAPICallError as e:\n",
        "        # Service side failure / quota / network\n",
        "        msg = f\"Speech Recognition service unavailable. Google API error: {e}\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "    except google_exceptions.RetryError as e:\n",
        "        msg = f\"Speech Recognition service unavailable (retry error): {e}\"\n",
        "        print(msg)\n",
        "        return msg\n",
        "    except Exception as e:\n",
        "        msg = f\"[Google error] {e}\"\n",
        "        print(msg)\n",
        "        return msg\n"
      ],
      "metadata": {
        "id": "xKU2KUrrIsQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(pred, ground_truth):\n",
        "    \"\"\"Return (WER, accuracy%) where accuracy% = (1 - WER) * 100\"\"\"\n",
        "    # If pred is an error message or empty, treat as full error\n",
        "    if not pred or pred.startswith(\"[\") or \"could not understand audio\" in pred.lower():\n",
        "        return 1.0, 0.0\n",
        "    try:\n",
        "        w = wer(ground_truth.lower(), pred.lower())\n",
        "        acc = max(0.0, 1.0 - w) * 100\n",
        "        return round(w, 3), round(acc, 2)\n",
        "    except Exception:\n",
        "        return 1.0, 0.0\n",
        "\n",
        "def setup_google_client():\n",
        "    \"\"\"Try to create a google speech client. If no credentials in env, prompt upload.\"\"\"\n",
        "    cred_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "    if cred_path and os.path.exists(cred_path):\n",
        "        try:\n",
        "            creds = service_account.Credentials.from_service_account_file(cred_path)\n",
        "            client = speech.SpeechClient(credentials=creds)\n",
        "            print(\"Using existing GOOGLE_APPLICATION_CREDENTIALS:\", cred_path)\n",
        "            return client, cred_path\n",
        "        except Exception as e:\n",
        "            print(\"Failed to use GOOGLE_APPLICATION_CREDENTIALS:\", e)\n",
        "\n",
        "    print(\"Google Cloud credentials not found in environment.\")\n",
        "    print(\"Please upload your Google Cloud service-account JSON key (Speech-to-Text API must be enabled).\")\n",
        "    uploaded = files.upload()\n",
        "    if not uploaded:\n",
        "        print(\"No credentials uploaded. Google Cloud recognition will be skipped.\")\n",
        "        return None, None\n",
        "    cred_filename = list(uploaded.keys())[0]\n",
        "    # Write file already present in Colab; set env var (optional)\n",
        "    os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = cred_filename\n",
        "    try:\n",
        "        creds = service_account.Credentials.from_service_account_file(cred_filename)\n",
        "        client = speech.SpeechClient(credentials=creds)\n",
        "        print(\"Google credentials loaded from\", cred_filename)\n",
        "        return client, cred_filename\n",
        "    except Exception as e:\n",
        "        print(\"Failed to create Google client from uploaded credentials:\", e)\n",
        "        return None, cred_filename\n"
      ],
      "metadata": {
        "id": "4xs_VsWEIu2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STEP A — Google Cloud setup (upload service-account JSON if you want to use Google Cloud STT).\")\n",
        "google_client, google_cred_file = setup_google_client()\n",
        "\n",
        "print(\"\\nSTEP B — Upload one or more audio files now (wav/flac/mp3).\")\n",
        "uploaded_files = files.upload()\n",
        "if not uploaded_files:\n",
        "    print(\"No audio files uploaded. Exiting.\")\n",
        "    raise SystemExit\n",
        "\n",
        "rows = []\n",
        "for fname in uploaded_files.keys():\n",
        "    print(f\"\\nProcessing file: {fname}\")\n",
        "    # Simulate 'Speak something...' feedback before processing (assignment requirement)\n",
        "    print(\"Speak something... (audio already recorded and uploaded)\")\n",
        "\n",
        "    # Ask for label and ground-truth for this file\n",
        "    label = input(f\"Enter label for {fname} (e.g., 'Clear male voice'): \").strip() or fname\n",
        "    ground_truth = input(f\"Enter GROUND TRUTH transcription for {fname} (exact text): \").strip()\n",
        "    if ground_truth == \"\":\n",
        "        print(\"No ground truth provided — using empty string (WER will be 1.0).\")\n",
        "\n",
        "    # Convert to proper wav for consistency\n",
        "    wav_for_stt = f\"stt_{fname}.wav\"\n",
        "    try:\n",
        "        convert_to_wav(fname, wav_for_stt)\n",
        "        print(\"Prepared audio (16kHz mono WAV).\")\n",
        "    except Exception as e:\n",
        "        print(\"Error converting audio:\", e)\n",
        "        rows.append({\n",
        "            \"Audio File\": fname,\n",
        "            \"Label\": label,\n",
        "            \"Ground Truth\": ground_truth,\n",
        "            \"Whisper Output\": f\"[Conversion error] {e}\",\n",
        "            \"Whisper WER\": 1.0, \"Whisper Acc %\": 0.0,\n",
        "            \"Vosk Output\": f\"[Conversion error] {e}\",\n",
        "            \"Vosk WER\": 1.0, \"Vosk Acc %\": 0.0,\n",
        "            \"Google Output\": f\"[Conversion error] {e}\",\n",
        "            \"Google WER\": 1.0, \"Google Acc %\": 0.0,\n",
        "        })\n",
        "        continue\n",
        "\n",
        "    # Run Whisper\n",
        "    whisper_out = run_whisper(wav_for_stt, model_name=WHISPER_MODEL)\n",
        "    w_wer, w_acc = calculate_accuracy(whisper_out, ground_truth)\n",
        "\n",
        "    # Run Vosk\n",
        "    vosk_out = run_vosk(wav_for_stt, model_path=VOSK_MODEL_PATH)\n",
        "    v_wer, v_acc = calculate_accuracy(vosk_out, ground_truth)\n",
        "\n",
        "    # Run Google Cloud (if client available)\n",
        "    if google_client is not None:\n",
        "        google_out = run_google_cloud(wav_for_stt, google_client)\n",
        "    else:\n",
        "        google_out = \"Speech Recognition service unavailable. Google credentials not provided.\"\n",
        "        print(google_out)\n",
        "    g_wer, g_acc = calculate_accuracy(google_out, ground_truth)\n",
        "\n",
        "    # choose best performing model for this file\n",
        "    best_model = max([(\"Whisper\", w_acc), (\"Vosk\", v_acc), (\"Google\", g_acc)], key=lambda x: x[1])[0]\n",
        "\n",
        "    rows.append({\n",
        "        \"Audio File\": fname,\n",
        "        \"Label\": label,\n",
        "        \"Ground Truth\": ground_truth,\n",
        "        \"Whisper Output\": whisper_out,\n",
        "        \"Whisper WER\": w_wer,\n",
        "        \"Whisper Acc %\": w_acc,\n",
        "        \"Vosk Output\": vosk_out,\n",
        "        \"Vosk WER\": v_wer,\n",
        "        \"Vosk Acc %\": v_acc,\n",
        "        \"Google Output\": google_out,\n",
        "        \"Google WER\": g_wer,\n",
        "        \"Google Acc %\": g_acc,\n",
        "        \"Best Model\": best_model\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(\"\\n--- Comparison Results ---\")\n",
        "display(df)\n",
        "\n",
        "df.to_csv(\"stt_comparison_results.csv\", index=False)\n",
        "with open(\"stt_comparison_results.md\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(df.to_markdown(index=False))\n",
        "print(\"Saved stt_comparison_results.csv and stt_comparison_results.md in the Colab filesystem.\")\n",
        "print(\"Download them using the Files sidebar or with files.download if you want.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qr1BIqDRI4eX",
        "outputId": "d244d340-b0c5-4ac0-e8e2-65d273cf6fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP A — Google Cloud setup (upload service-account JSON if you want to use Google Cloud STT).\n",
            "Using existing GOOGLE_APPLICATION_CREDENTIALS: nodal-ivy-473710-r7-afc93e14c3af.json\n",
            "\n",
            "STEP B — Upload one or more audio files now (wav/flac/mp3).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06b55507-716f-43c5-96bf-50740617fb47\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06b55507-716f-43c5-96bf-50740617fb47\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Soft Voice .m4a to Soft Voice  (1).m4a\n",
            "Saving Clear Male.m4a to Clear Male (2).m4a\n",
            "Saving Clear Female.m4a to Clear Female (1).m4a\n",
            "Saving noisy background .m4a to noisy background  (1).m4a\n",
            "Saving Fast Voice.m4a to Fast Voice (1).m4a\n",
            "\n",
            "Processing file: Soft Voice  (1).m4a\n",
            "Speak something... (audio already recorded and uploaded)\n",
            "Enter label for Soft Voice  (1).m4a (e.g., 'Clear male voice'): Soft Voice\n",
            "Enter GROUND TRUTH transcription for Soft Voice  (1).m4a (exact text): terms and conditions\n",
            "Prepared audio (16kHz mono WAV).\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech successfully converted to text! (Whisper)\n",
            "Recognizing with Vosk...\n",
            "Speech successfully converted to text! (Vosk)\n",
            "Recognizing with Google Cloud Speech-to-Text...\n",
            "Speech successfully converted to text! (Google Cloud)\n",
            "\n",
            "Processing file: Clear Male (2).m4a\n",
            "Speak something... (audio already recorded and uploaded)\n",
            "Enter label for Clear Male (2).m4a (e.g., 'Clear male voice'):  Clear Male\n",
            "Enter GROUND TRUTH transcription for Clear Male (2).m4a (exact text):  Hello there, what can i do to lend a hand\n",
            "Prepared audio (16kHz mono WAV).\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech successfully converted to text! (Whisper)\n",
            "Recognizing with Vosk...\n",
            "Speech successfully converted to text! (Vosk)\n",
            "Recognizing with Google Cloud Speech-to-Text...\n",
            "Speech successfully converted to text! (Google Cloud)\n",
            "\n",
            "Processing file: Clear Female (1).m4a\n",
            "Speak something... (audio already recorded and uploaded)\n",
            "Enter label for Clear Female (1).m4a (e.g., 'Clear male voice'): Clear Female\n",
            "Enter GROUND TRUTH transcription for Clear Female (1).m4a (exact text): Can i read something\n",
            "Prepared audio (16kHz mono WAV).\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech successfully converted to text! (Whisper)\n",
            "Recognizing with Vosk...\n",
            "Speech successfully converted to text! (Vosk)\n",
            "Recognizing with Google Cloud Speech-to-Text...\n",
            "Speech successfully converted to text! (Google Cloud)\n",
            "\n",
            "Processing file: noisy background  (1).m4a\n",
            "Speak something... (audio already recorded and uploaded)\n",
            "Enter label for noisy background  (1).m4a (e.g., 'Clear male voice'): noisy background\n",
            "Enter GROUND TRUTH transcription for noisy background  (1).m4a (exact text): \n",
            "No ground truth provided — using empty string (WER will be 1.0).\n",
            "Prepared audio (16kHz mono WAV).\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech Recognition could not understand audio. Please try speaking more clearly.\n",
            "Recognizing with Vosk...\n",
            "Speech Recognition could not understand audio. Please try speaking more clearly.\n",
            "Recognizing with Google Cloud Speech-to-Text...\n",
            "Speech Recognition could not understand audio. Please try speaking more clearly.\n",
            "\n",
            "Processing file: Fast Voice (1).m4a\n",
            "Speak something... (audio already recorded and uploaded)\n",
            "Enter label for Fast Voice (1).m4a (e.g., 'Clear male voice'): Fast Voice\n",
            "Enter GROUND TRUTH transcription for Fast Voice (1).m4a (exact text): \n",
            "No ground truth provided — using empty string (WER will be 1.0).\n",
            "Prepared audio (16kHz mono WAV).\n",
            "Recognizing with Whisper...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speech successfully converted to text! (Whisper)\n",
            "Recognizing with Vosk...\n",
            "Speech successfully converted to text! (Vosk)\n",
            "Recognizing with Google Cloud Speech-to-Text...\n",
            "Speech successfully converted to text! (Google Cloud)\n",
            "\n",
            "--- Comparison Results ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                  Audio File             Label  \\\n",
              "0        Soft Voice  (1).m4a        Soft Voice   \n",
              "1         Clear Male (2).m4a        Clear Male   \n",
              "2       Clear Female (1).m4a      Clear Female   \n",
              "3  noisy background  (1).m4a  noisy background   \n",
              "4         Fast Voice (1).m4a        Fast Voice   \n",
              "\n",
              "                                Ground Truth  \\\n",
              "0                       terms and conditions   \n",
              "1  Hello there, what can i do to lend a hand   \n",
              "2                       Can i read something   \n",
              "3                                              \n",
              "4                                              \n",
              "\n",
              "                                      Whisper Output  Whisper WER  \\\n",
              "0                               Terms and Conditions          0.0   \n",
              "1         Hello there, what can I do to lend a hand?          0.1   \n",
              "2                               Can I eat something?          0.5   \n",
              "3  Speech Recognition could not understand audio....          1.0   \n",
              "4                             Hi there, how are you?          5.0   \n",
              "\n",
              "   Whisper Acc %                                        Vosk Output  Vosk WER  \\\n",
              "0          100.0                               terms and conditions       0.0   \n",
              "1           90.0        hello there were going to do to lend a hand       0.4   \n",
              "2           50.0                               and i need something       0.5   \n",
              "3            0.0  Speech Recognition could not understand audio....       1.0   \n",
              "4            0.0                          hi there how i don't know       6.0   \n",
              "\n",
              "   Vosk Acc %                                      Google Output  Google WER  \\\n",
              "0       100.0                              Terms and conditions.       0.333   \n",
              "1        60.0         Hello there. What can I do to lend a hand?       0.200   \n",
              "2        50.0                              Can I read something?       0.250   \n",
              "3         0.0  Speech Recognition could not understand audio....       1.000   \n",
              "4         0.0                             Hi there. How are you?       5.000   \n",
              "\n",
              "   Google Acc % Best Model  \n",
              "0         66.67    Whisper  \n",
              "1         80.00    Whisper  \n",
              "2         75.00     Google  \n",
              "3          0.00    Whisper  \n",
              "4          0.00    Whisper  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ada1195d-9b80-4e77-b65a-29091fd65b56\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Audio File</th>\n",
              "      <th>Label</th>\n",
              "      <th>Ground Truth</th>\n",
              "      <th>Whisper Output</th>\n",
              "      <th>Whisper WER</th>\n",
              "      <th>Whisper Acc %</th>\n",
              "      <th>Vosk Output</th>\n",
              "      <th>Vosk WER</th>\n",
              "      <th>Vosk Acc %</th>\n",
              "      <th>Google Output</th>\n",
              "      <th>Google WER</th>\n",
              "      <th>Google Acc %</th>\n",
              "      <th>Best Model</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Soft Voice  (1).m4a</td>\n",
              "      <td>Soft Voice</td>\n",
              "      <td>terms and conditions</td>\n",
              "      <td>Terms and Conditions</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>terms and conditions</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>Terms and conditions.</td>\n",
              "      <td>0.333</td>\n",
              "      <td>66.67</td>\n",
              "      <td>Whisper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clear Male (2).m4a</td>\n",
              "      <td>Clear Male</td>\n",
              "      <td>Hello there, what can i do to lend a hand</td>\n",
              "      <td>Hello there, what can I do to lend a hand?</td>\n",
              "      <td>0.1</td>\n",
              "      <td>90.0</td>\n",
              "      <td>hello there were going to do to lend a hand</td>\n",
              "      <td>0.4</td>\n",
              "      <td>60.0</td>\n",
              "      <td>Hello there. What can I do to lend a hand?</td>\n",
              "      <td>0.200</td>\n",
              "      <td>80.00</td>\n",
              "      <td>Whisper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Clear Female (1).m4a</td>\n",
              "      <td>Clear Female</td>\n",
              "      <td>Can i read something</td>\n",
              "      <td>Can I eat something?</td>\n",
              "      <td>0.5</td>\n",
              "      <td>50.0</td>\n",
              "      <td>and i need something</td>\n",
              "      <td>0.5</td>\n",
              "      <td>50.0</td>\n",
              "      <td>Can I read something?</td>\n",
              "      <td>0.250</td>\n",
              "      <td>75.00</td>\n",
              "      <td>Google</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>noisy background  (1).m4a</td>\n",
              "      <td>noisy background</td>\n",
              "      <td></td>\n",
              "      <td>Speech Recognition could not understand audio....</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Speech Recognition could not understand audio....</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Speech Recognition could not understand audio....</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Whisper</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Fast Voice (1).m4a</td>\n",
              "      <td>Fast Voice</td>\n",
              "      <td></td>\n",
              "      <td>Hi there, how are you?</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>hi there how i don't know</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Hi there. How are you?</td>\n",
              "      <td>5.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Whisper</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ada1195d-9b80-4e77-b65a-29091fd65b56')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ada1195d-9b80-4e77-b65a-29091fd65b56 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ada1195d-9b80-4e77-b65a-29091fd65b56');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-afb56ab2-0242-4079-81cb-2a24e5270667\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afb56ab2-0242-4079-81cb-2a24e5270667')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-afb56ab2-0242-4079-81cb-2a24e5270667 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_7c46dd1f-2aef-4a02-8816-dbac76410d1e\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_7c46dd1f-2aef-4a02-8816-dbac76410d1e button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Audio File\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Clear Male (2).m4a\",\n          \"Fast Voice (1).m4a\",\n          \"Clear Female (1).m4a\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Label\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Clear Male\",\n          \"Fast Voice\",\n          \"Clear Female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ground Truth\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Hello there, what can i do to lend a hand\",\n          \"\",\n          \"terms and conditions\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Whisper Output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Hello there, what can I do to lend a hand?\",\n          \"Hi there, how are you?\",\n          \"Can I eat something?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Whisper WER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.0945166506857853,\n        \"min\": 0.0,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.1,\n          5.0,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Whisper Acc %\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 47.644516998286385,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          90.0,\n          0.0,\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vosk Output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"hello there were going to do to lend a hand\",\n          \"hi there how i don't know\",\n          \"and i need something\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vosk WER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.496397404260788,\n        \"min\": 0.0,\n        \"max\": 6.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.4,\n          6.0,\n          0.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vosk Acc %\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 42.661458015403085,\n        \"min\": 0.0,\n        \"max\": 100.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          60.0,\n          0.0,\n          100.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Google Output\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Hello there. What can I do to lend a hand?\",\n          \"Hi there. How are you?\",\n          \"Can I read something?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Google WER\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.0622543005167913,\n        \"min\": 0.2,\n        \"max\": 5.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.2,\n          5.0,\n          0.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Google Acc %\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40.75037153204864,\n        \"min\": 0.0,\n        \"max\": 80.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          80.0,\n          0.0,\n          66.67\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Best Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Google\",\n          \"Whisper\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved stt_comparison_results.csv and stt_comparison_results.md in the Colab filesystem.\n",
            "Download them using the Files sidebar or with files.download if you want.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📝 Report: Speech-to-Text System Execution and Observations\n",
        "\n",
        "## 1. System Execution\n",
        "- Implemented a **Python-based Speech-to-Text application** in Colab using:\n",
        "  - **Whisper (offline)**\n",
        "  - **Vosk (offline)**\n",
        "  - **Google Cloud Speech-to-Text API (online)**\n",
        "- The system provided user feedback at each stage:\n",
        "  - *“Speak something...”* before recognition  \n",
        "  - *“Recognizing...”* during processing  \n",
        "  - *“Speech successfully converted to text!”* on success  \n",
        "  - Meaningful error messages when speech was unclear or service was unavailable.  \n",
        "- All models were tested on multiple audio scenarios: clear male/female voices, soft voice, fast speech, and noisy background.  \n",
        "- Outputs were compared against **ground truth transcriptions** using **Word Error Rate (WER)** and **Accuracy %**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Observations & Comparative Analysis\n",
        "From the results table:\n",
        "\n",
        "| Scenario              | Best Model          | Key Observations |\n",
        "|-----------------------|---------------------|------------------|\n",
        "| **Soft Voice**        | Whisper & Vosk (100%) | Both handled soft voice well; Google slightly mispunctuated → lower accuracy. |\n",
        "| **Clear Male Voice**  | Whisper (90%)       | Whisper produced the closest transcription; Vosk dropped words, Google had minor segmentation errors. |\n",
        "| **Clear Female Voice**| Google (75%)        | Google captured the sentence correctly; Whisper and Vosk confused words. |\n",
        "| **Noisy Background**  | None (0%)           | All models failed to recognize due to strong noise; each returned an error message as expected. |\n",
        "| **Fast Voice**        | Whisper (best among low scores) | All models struggled; Whisper preserved more structure, but accuracy remained very low. |\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Key Findings\n",
        "- **Whisper**: Most consistent across conditions; performed best on soft and male voices.  \n",
        "- **Vosk**: Performed well on soft voice but struggled with clarity and speed.  \n",
        "- **Google API**: Strong on clear female voice and natural phrasing, but weaker on soft voice.  \n",
        "- **Noise Sensitivity**: All models failed under high background noise, validating the need for preprocessing (e.g., noise reduction).  \n",
        "- **Fast Speech**: Remains a challenge for all systems.  \n",
        "\n",
        "---\n",
        "\n",
        "## 4. Conclusion\n",
        "- **No single model is perfect**; each has strengths:\n",
        "  - Whisper → robust on soft/male voices.  \n",
        "  - Google → better on female/clear speech.  \n",
        "  - Vosk → lightweight but less accurate overall.  \n",
        "- Combining models or applying **noise reduction & speech enhancement** before recognition could improve overall performance.  \n",
        "- Error-handling was effective, ensuring user-friendly messages when recognition failed.  \n"
      ],
      "metadata": {
        "id": "AqDcWyxVPUVW"
      }
    }
  ]
}